{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from keras.models import Sequential, Model\n",
    "from keras import regularizers\n",
    "from keras import optimizers, initializers\n",
    "from keras import callbacks\n",
    "import sklearn.metrics\n",
    "from keras.layers import Activation,Dense, Input, GlobalMaxPooling1D, BatchNormalization, Dropout,Conv1D, MaxPooling1D, Embedding, concatenate,Reshape\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import math\n",
    "from keras.initializers import Constant\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86400\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (43,61,62) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59881, 96)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./pairs/listings.csv') \n",
    "df = shuffle(df, random_state=0)\n",
    "print(df.shape)\n",
    "\n",
    "text_cols = ['name', 'summary', 'space', 'description', 'neighborhood_overview', 'notes', 'transit', 'access', \n",
    "             'interaction', 'house_rules', 'host_name', 'host_about']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open( 'glove.6B.50d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_date(cols):\n",
    "    num_data = cols.shape[0]\n",
    "    #print(num_data)\n",
    "    dates = pd.to_datetime(cols)\n",
    "    min_date = pd.to_datetime(date.today())\n",
    "    for col in dates:\n",
    "        if not isinstance(col, float):\n",
    "            min_date = min(min_date, col)\n",
    "    mean_date = dates.mean()\n",
    "    \n",
    "    dif = pd.to_timedelta([mean_date - min_date]).astype('timedelta64[h]')[0]\n",
    "    arr = np.zeros((num_data, 1))\n",
    "    for i, col in enumerate(dates):\n",
    "        arr[i] = pd.to_timedelta([col - min_date]).astype('timedelta64[h]')[0]\n",
    "        if np.isnan(arr[i]):\n",
    "            arr[i] = dif\n",
    "\n",
    "    # print (np.min(arr), np.max(arr))\n",
    "    return arr\n",
    "\n",
    "def clean_host_response_rate(host_response_rate, num_data):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    for col in host_response_rate:\n",
    "        if not isinstance(col, float):\n",
    "            total += float(col.strip('%'))\n",
    "            count += 1\n",
    "\n",
    "    arr = np.zeros(num_data)\n",
    "    mean = total / count\n",
    "    print (host_response_rate.name, 'mean is ', mean)\n",
    "    for i, col in enumerate(host_response_rate):\n",
    "        if not isinstance(col, float):\n",
    "            arr[i] += float(col.strip('%'))\n",
    "        else:\n",
    "            assert(math.isnan(col))\n",
    "            arr[i] = mean\n",
    "    return arr\n",
    "\n",
    "def clean_price(price, num_data):\n",
    "    '''\n",
    "    total = 0\n",
    "    count = 0\n",
    "    all_count=0\n",
    "    for col in price:\n",
    "        all_count+=1\n",
    "        if not isinstance(col, float):\n",
    "            total += float(col.strip('$').replace(',', ''))\n",
    "            count += 1\n",
    "    \n",
    "    \n",
    "    mean = total / count\n",
    "    print (price.name, 'mean is ', mean)\n",
    "    print(all_count-count)\n",
    "    '''\n",
    "    arr = np.zeros(num_data)\n",
    "    for i, col in enumerate(price):\n",
    "        if not isinstance(col, float):\n",
    "            arr[i] += float(col.strip('$').replace(',', ''))\n",
    "        else:\n",
    "            assert(math.isnan(col))\n",
    "            arr[i] = 0\n",
    "    return arr\n",
    "\n",
    "def check_nan(cols):\n",
    "    for col in cols:\n",
    "        #print (col)\n",
    "        if np.isnan(col):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def to_np_array_fill_NA_mean(cols):\n",
    "    print (cols.name, 'mean is ', np.nanmean(np.array(cols)))\n",
    "    return np.array(cols.fillna(np.nanmean(np.array(cols))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host_response_rate mean is  91.81647347518647\n",
      "num_data 59881\n",
      "host_listings_count mean is  8.456983949359477\n",
      "host_total_listings_count mean is  8.456983949359477\n",
      "bathrooms mean is  1.108602959618761\n",
      "bedrooms mean is  1.087790279705081\n",
      "beds mean is  1.671634284947949\n",
      "square_feet mean is  383.66568483063327\n",
      "review_scores_rating mean is  92.82449394024498\n",
      "review_scores_accuracy mean is  9.574485115957078\n",
      "review_scores_cleanliness mean is  9.203351713698778\n",
      "review_scores_checkin mean is  9.688236058191894\n",
      "review_scores_communication mean is  9.719251568245728\n",
      "review_scores_location mean is  9.59474379234949\n",
      "review_scores_value mean is  9.273250779355733\n",
      "reviews_per_month mean is  1.163733144822638\n",
      "(59881, 30)\n"
     ]
    }
   ],
   "source": [
    "num_data = df.shape[0]\n",
    "\n",
    "features = ['host_listings_count', 'host_total_listings_count', 'latitude', 'longitude', \n",
    "      'accommodates', 'bathrooms', 'bedrooms', 'beds', 'square_feet',     \n",
    "      'guests_included', 'minimum_nights', 'maximum_nights', 'availability_30', 'availability_60', \n",
    "      'availability_90', 'availability_365', 'number_of_reviews', 'review_scores_rating', 'review_scores_accuracy', \n",
    "      'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', \n",
    "      'review_scores_value', 'calculated_host_listings_count', 'reviews_per_month']\n",
    "\n",
    "      \n",
    "      \n",
    "price_features = ['security_deposit', 'cleaning_fee', 'extra_people','price'] \n",
    "\n",
    "arr = np.zeros((len(features) + len(price_features) + 1, num_data))\n",
    "# check_nan(df['extra_people'])\n",
    "\n",
    "host_response_rate = clean_host_response_rate(df['host_response_rate'], num_data)\n",
    "arr[0] = host_response_rate\n",
    "print(\"num_data\", num_data)\n",
    "i = 0\n",
    "for feature in features:\n",
    "    i += 1\n",
    "    if check_nan(df[feature]):\n",
    "        arr[i] = to_np_array_fill_NA_mean(df[feature])\n",
    "    else:\n",
    "        arr[i] = np.array(df[feature])\n",
    "    \n",
    "\n",
    "for feature in price_features:\n",
    "    i += 1\n",
    "    arr[i] = clean_price(df[feature], num_data)\n",
    "\n",
    "label = arr[-1]\n",
    "arr = arr[:-1].T\n",
    "#arr = arr.T\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainX_nontext = np.loadtxt(open('./model/trainX_nontext.csv','r'), delimiter = ',', skiprows=0)\n",
    "trainy_nontext = np.loadtxt(open('./model/trainy_nontext.csv','r'), delimiter = ',', skiprows=0)\n",
    "\n",
    "devX_nontext = np.loadtxt(open('./model/devX_nontext.csv','r'), delimiter = ',', skiprows=0)\n",
    "devy_nontext = np.loadtxt(open('./model/devy_nontext.csv','r'), delimiter = ',', skiprows=0)\n",
    "\n",
    "testX_nontext = np.loadtxt(open('./model/testX_nontext.csv','r'), delimiter = ',', skiprows=0)\n",
    "testy_nontext = np.loadtxt(open('./model/testy_nontext.csv','r'), delimiter = ',', skiprows=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41909, 381)\n",
      "(41909, 1)\n",
      "(41909, 1)\n",
      "(11974, 1)\n"
     ]
    }
   ],
   "source": [
    "print(trainX_nontext.shape)\n",
    "print(trainy_nontext.shape)\n",
    "trainy_nontext = trainy_nontext.reshape((-1,1))\n",
    "devy_nontext = devy_nontext.reshape((-1,1))\n",
    "testy_nontext = testy_nontext.reshape((-1,1))\n",
    "print(trainy_nontext.shape)\n",
    "print(devy_nontext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43228441\n",
      "722.0384332720895\n"
     ]
    }
   ],
   "source": [
    "mask = label > 0\n",
    "texts = df['description'][mask].values.astype('U')\n",
    "num = 0\n",
    "n = 0\n",
    "for i in texts:\n",
    "    l = list(i)\n",
    "    len_l = len(l)\n",
    "    num = num + len_l\n",
    "    n = num/59870\n",
    "print(num)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65633 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "mask = label > 0\n",
    "texts = df['description'][mask].values.astype('U')\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = data.shape[0]\n",
    "train_num = int(num_data * 0.7)\n",
    "dev_num = int(num_data * 0.9)\n",
    "test_num = num_data - train_num\n",
    "\n",
    "trainX_text = data[:train_num]\n",
    "devX_text = data[train_num:dev_num]\n",
    "testX_text = data[dev_num:]\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(20000, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, 50))\n",
    "for word, i in word_index.items():\n",
    "    if i >= 20000:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59870, 300)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41909, 300)\n",
      "(11974, 300)\n",
      "(5987, 300)\n",
      "(59870, 300)\n",
      "59870\n",
      "59870\n"
     ]
    }
   ],
   "source": [
    "print(trainX_text.shape)\n",
    "print(devX_text.shape)\n",
    "print(testX_text.shape)\n",
    "print(data.shape)\n",
    "print(len(texts))\n",
    "print(num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41909 samples, validate on 11974 samples\n",
      "Epoch 1/100\n",
      "41909/41909 [==============================] - 113s 3ms/step - loss: 4.0348 - accuracy: 0.0000e+00 - mse: 1.6607 - val_loss: 2.3183 - val_accuracy: 0.0000e+00 - val_mse: 0.6577\n",
      "Epoch 2/100\n",
      "41909/41909 [==============================] - 110s 3ms/step - loss: 1.7370 - accuracy: 0.0000e+00 - mse: 0.2343 - val_loss: 1.6481 - val_accuracy: 0.0000e+00 - val_mse: 0.2589\n",
      "Epoch 3/100\n",
      "41909/41909 [==============================] - 123s 3ms/step - loss: 1.4990 - accuracy: 0.0000e+00 - mse: 0.1775 - val_loss: 1.4111 - val_accuracy: 0.0000e+00 - val_mse: 0.1465\n",
      "Epoch 4/100\n",
      "41909/41909 [==============================] - 149s 4ms/step - loss: 1.3696 - accuracy: 0.0000e+00 - mse: 0.1507 - val_loss: 1.3854 - val_accuracy: 0.0000e+00 - val_mse: 0.2067\n",
      "Epoch 5/100\n",
      "41909/41909 [==============================] - 135s 3ms/step - loss: 1.2762 - accuracy: 0.0000e+00 - mse: 0.1368 - val_loss: 1.3153 - val_accuracy: 0.0000e+00 - val_mse: 0.2144\n",
      "Epoch 6/100\n",
      "41909/41909 [==============================] - 136s 3ms/step - loss: 1.2023 - accuracy: 0.0000e+00 - mse: 0.1370 - val_loss: 1.2340 - val_accuracy: 0.0000e+00 - val_mse: 0.2038\n",
      "Epoch 7/100\n",
      "41909/41909 [==============================] - 136s 3ms/step - loss: 1.1213 - accuracy: 0.0000e+00 - mse: 0.1262 - val_loss: 1.1158 - val_accuracy: 0.0000e+00 - val_mse: 0.1548\n",
      "Epoch 8/100\n",
      "41909/41909 [==============================] - 137s 3ms/step - loss: 1.0508 - accuracy: 0.0000e+00 - mse: 0.1231 - val_loss: 1.0138 - val_accuracy: 0.0000e+00 - val_mse: 0.1203\n",
      "Epoch 9/100\n",
      "41909/41909 [==============================] - 136s 3ms/step - loss: 0.9836 - accuracy: 0.0000e+00 - mse: 0.1224 - val_loss: 0.9805 - val_accuracy: 0.0000e+00 - val_mse: 0.1511\n",
      "Epoch 10/100\n",
      "41909/41909 [==============================] - 136s 3ms/step - loss: 0.9170 - accuracy: 0.0000e+00 - mse: 0.1194 - val_loss: 0.9575 - val_accuracy: 0.0000e+00 - val_mse: 0.1932\n",
      "Epoch 11/100\n",
      "41909/41909 [==============================] - 142s 3ms/step - loss: 0.8506 - accuracy: 0.0000e+00 - mse: 0.1171 - val_loss: 0.8258 - val_accuracy: 0.0000e+00 - val_mse: 0.1228\n",
      "Epoch 12/100\n",
      "41909/41909 [==============================] - 166s 4ms/step - loss: 0.7863 - accuracy: 0.0000e+00 - mse: 0.1134 - val_loss: 0.7905 - val_accuracy: 0.0000e+00 - val_mse: 0.1463\n",
      "Epoch 13/100\n",
      "41909/41909 [==============================] - 169s 4ms/step - loss: 0.7284 - accuracy: 0.0000e+00 - mse: 0.1135 - val_loss: 0.6971 - val_accuracy: 0.0000e+00 - val_mse: 0.1110\n",
      "Epoch 14/100\n",
      "41909/41909 [==============================] - 224s 5ms/step - loss: 0.6677 - accuracy: 0.0000e+00 - mse: 0.1087 - val_loss: 0.6465 - val_accuracy: 0.0000e+00 - val_mse: 0.1143\n",
      "Epoch 15/100\n",
      "41909/41909 [==============================] - 277s 7ms/step - loss: 0.6151 - accuracy: 0.0000e+00 - mse: 0.1088 - val_loss: 0.6191 - val_accuracy: 0.0000e+00 - val_mse: 0.1382\n",
      "Epoch 16/100\n",
      "41909/41909 [==============================] - 231s 6ms/step - loss: 0.5638 - accuracy: 0.0000e+00 - mse: 0.1069 - val_loss: 0.5533 - val_accuracy: 0.0000e+00 - val_mse: 0.1205\n",
      "Epoch 17/100\n",
      "41909/41909 [==============================] - 207s 5ms/step - loss: 0.5157 - accuracy: 0.0000e+00 - mse: 0.1060 - val_loss: 0.5173 - val_accuracy: 0.0000e+00 - val_mse: 0.1301\n",
      "Epoch 18/100\n",
      "41909/41909 [==============================] - 210s 5ms/step - loss: 0.4715 - accuracy: 0.0000e+00 - mse: 0.1058 - val_loss: 0.5208 - val_accuracy: 0.0000e+00 - val_mse: 0.1762\n",
      "0.10950135381969013 0.13355662797024417 0.7245206974781864 0.6671167476665048\n",
      "train score :  0.7245206974781864\n",
      "test score :  0.6671167476665048\n",
      "train mse :  0.10950135381969013\n",
      "test mse :  0.13355662797024417\n",
      "Run time :  2946\n"
     ]
    }
   ],
   "source": [
    "# GloVe\n",
    "init = initializers.glorot_uniform(seed=0)\n",
    "\n",
    "# non-text\n",
    "nontext_input = Input(shape=(381,))\n",
    "d1 = Dense(300, activation='relu', kernel_initializer=init, kernel_regularizer=regularizers.l2(0.01))(nontext_input)\n",
    "d2 = Dense(200, activation='relu')(d1)\n",
    "d3 = Dense(100, activation='relu')(d2)\n",
    "d4 = Dense(1)(d3)\n",
    "\n",
    "# text\n",
    "text_input = Input(shape=(300,), dtype='int32')\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            50,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=300,\n",
    "                            trainable=False)(text_input)\n",
    "# train a 1D convnet with global maxpooling\n",
    "x = Conv1D(128, 5)(embedding_layer)\n",
    "bn = BatchNormalization()(x)\n",
    "act = Activation('relu')(bn)\n",
    "dropout = Dropout(0.2)(act)\n",
    "x = MaxPooling1D(5)(dropout)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "dense1 = Dense(128, kernel_initializer=init, kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "dense2 = Dense(64, activation='relu')(dense1)\n",
    "dense3 = Dense(1)(dense2)\n",
    "\n",
    "merger = concatenate([d4,dense3])\n",
    "\n",
    "out = Dense(1)(merger)\n",
    "\n",
    "model = Model(inputs=[nontext_input, text_input], outputs=out)\n",
    "\n",
    "model.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"accuracy\",\"mse\"])\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "model.fit([trainX_nontext,trainX_text], trainy_nontext, epochs=100, batch_size=200, validation_data=[[devX_nontext,devX_text],devy_nontext],  callbacks=[earlystopping])\n",
    "endtime = datetime.datetime.now()\n",
    "\n",
    "train_predict = model.predict([trainX_nontext,trainX_text])\n",
    "\n",
    "test_predict = model.predict([testX_nontext,testX_text])\n",
    "\n",
    "mse_train = sklearn.metrics.mean_squared_error(trainy_nontext, train_predict)\n",
    "mse_test = sklearn.metrics.mean_squared_error(testy_nontext, test_predict)\n",
    "\n",
    "r2_train = sklearn.metrics.r2_score(trainy_nontext, train_predict)\n",
    "r2_test = sklearn.metrics.r2_score(testy_nontext, test_predict)\n",
    "\n",
    "print(mse_train,mse_test,r2_train,r2_test)\n",
    "\n",
    "print('train score : ',r2_train)\n",
    "print('test score : ',r2_test)\n",
    "print('train mse : ', mse_train)\n",
    "print('test mse : ', mse_test)\n",
    "print(\"Run time : \",(endtime - starttime).seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_text = np.loadtxt(open('./model/trainX_text.csv','r'), delimiter = ',', skiprows=0)\n",
    "trainy_text = np.loadtxt(open('./model/trainy_text.csv','r'), delimiter = ',', skiprows=0)\n",
    "\n",
    "devX_text = np.loadtxt(open('./model/devX_text.csv','r'), delimiter = ',', skiprows=0)\n",
    "devy_text = np.loadtxt(open('./model/devy_text.csv','r'), delimiter = ',', skiprows=0)\n",
    "\n",
    "testX_text = np.loadtxt(open('./model/testX_text.csv','r'), delimiter = ',', skiprows=0)\n",
    "testy_text = np.loadtxt(open('./model/testy_text.csv','r'), delimiter = ',', skiprows=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.74886466 -0.9793375  -0.09167473 ...  1.03600904 -0.30108228\n",
      "   0.18866085]\n",
      " [-0.78240987  1.18475344 -0.0916559  ...  0.75598606 -1.56942359\n",
      "   0.12484921]\n",
      " [-1.08085512  0.80438886 -0.09164898 ... -0.4857522  -0.38721802\n",
      "  -0.44720533]\n",
      " ...\n",
      " [-0.48964957 -0.64927438 -0.09167856 ...  1.03885743  1.1014024\n",
      "  -2.2090919 ]\n",
      " [ 0.26539369 -0.63822341 -0.09166126 ... -1.43413554  1.17728637\n",
      "   0.91319473]\n",
      " [-0.5965585  -0.64495683 -0.09163774 ...  0.64721921 -0.78980207\n",
      "  -0.45136055]]\n"
     ]
    }
   ],
   "source": [
    "print(trainX_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41909 samples, validate on 11974 samples\n",
      "Epoch 1/100\n",
      "41909/41909 [==============================] - 3s 69us/step - loss: 2.8351 - accuracy: 0.0000e+00 - mse: 0.6778 - val_loss: 1.0433 - val_accuracy: 0.0000e+00 - val_mse: 0.1647\n",
      "Epoch 2/100\n",
      "41909/41909 [==============================] - 2s 48us/step - loss: 0.6241 - accuracy: 0.0000e+00 - mse: 0.1399 - val_loss: 0.3572 - val_accuracy: 0.0000e+00 - val_mse: 0.1206\n",
      "Epoch 3/100\n",
      "41909/41909 [==============================] - 2s 50us/step - loss: 0.2696 - accuracy: 0.0000e+00 - mse: 0.1244 - val_loss: 0.2089 - val_accuracy: 0.0000e+00 - val_mse: 0.1208\n",
      "Epoch 4/100\n",
      "41909/41909 [==============================] - 3s 63us/step - loss: 0.1871 - accuracy: 0.0000e+00 - mse: 0.1228 - val_loss: 0.1704 - val_accuracy: 0.0000e+00 - val_mse: 0.1215\n",
      "Epoch 5/100\n",
      "41909/41909 [==============================] - 2s 53us/step - loss: 0.1666 - accuracy: 0.0000e+00 - mse: 0.1237 - val_loss: 0.1617 - val_accuracy: 0.0000e+00 - val_mse: 0.1243\n",
      "Epoch 6/100\n",
      "41909/41909 [==============================] - 2s 48us/step - loss: 0.1549 - accuracy: 0.0000e+00 - mse: 0.1203 - val_loss: 0.1481 - val_accuracy: 0.0000e+00 - val_mse: 0.1161\n",
      "Epoch 7/100\n",
      "41909/41909 [==============================] - 2s 48us/step - loss: 0.1530 - accuracy: 0.0000e+00 - mse: 0.1222 - val_loss: 0.1458 - val_accuracy: 0.0000e+00 - val_mse: 0.1166\n",
      "Epoch 8/100\n",
      "41909/41909 [==============================] - 2s 50us/step - loss: 0.1496 - accuracy: 0.0000e+00 - mse: 0.1206 - val_loss: 0.1434 - val_accuracy: 0.0000e+00 - val_mse: 0.1156\n",
      "Epoch 9/100\n",
      "41909/41909 [==============================] - 2s 50us/step - loss: 0.1440 - accuracy: 0.0000e+00 - mse: 0.1172 - val_loss: 0.1383 - val_accuracy: 0.0000e+00 - val_mse: 0.1125\n",
      "Epoch 10/100\n",
      "41909/41909 [==============================] - 3s 64us/step - loss: 0.1433 - accuracy: 0.0000e+00 - mse: 0.1181 - val_loss: 0.1435 - val_accuracy: 0.0000e+00 - val_mse: 0.1190\n",
      "0.10700737021403643 0.11496213579814898 0.730794965696985 0.7134625945466448\n",
      "train score :  0.730794965696985\n",
      "test score :  0.7134625945466448\n",
      "train mse :  0.10700737021403643\n",
      "test mse :  0.11496213579814898\n",
      "Run time :  32\n"
     ]
    }
   ],
   "source": [
    "init = initializers.glorot_uniform(seed=0)\n",
    "\n",
    "# non-text\n",
    "nontext_input = Input(shape=(381,))\n",
    "d1 = Dense(300, activation='relu', kernel_initializer=init, kernel_regularizer=regularizers.l2(0.01))(nontext_input)\n",
    "d2 = Dense(200, activation='relu')(d1)\n",
    "d3 = Dense(100, activation='relu')(d2)\n",
    "d4 = Dense(1)(d3)\n",
    "\n",
    "# text\n",
    "text_input = Input(shape=(300,))\n",
    "dense1 = Dense(128, activation='relu', kernel_initializer=init, kernel_regularizer=regularizers.l2(0.01))(text_input)\n",
    "dense2 = Dense(64, activation='relu')(dense1)\n",
    "dense3 = Dense(1)(dense2)\n",
    "\n",
    "merger = concatenate([d4,dense3])\n",
    "\n",
    "out = Dense(1)(merger)\n",
    "\n",
    "model = Model(inputs=[nontext_input, text_input], outputs=out)\n",
    "\n",
    "model.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"accuracy\",\"mse\"])\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "model.fit([trainX_nontext,trainX_text], trainy_nontext, epochs=100, batch_size=200, validation_data=[[devX_nontext,devX_text],devy_nontext],  callbacks=[earlystopping])\n",
    "endtime = datetime.datetime.now()\n",
    "\n",
    "train_predict = model.predict([trainX_nontext,trainX_text],batch_size=200,callbacks=[earlystopping])\n",
    "\n",
    "test_predict = model.predict([testX_nontext,testX_text],batch_size=200,callbacks=[earlystopping])\n",
    "\n",
    "mse_train = sklearn.metrics.mean_squared_error(trainy_nontext, train_predict)\n",
    "mse_test = sklearn.metrics.mean_squared_error(testy_nontext, test_predict)\n",
    "\n",
    "r2_train = sklearn.metrics.r2_score(trainy_nontext, train_predict)\n",
    "r2_test = sklearn.metrics.r2_score(testy_nontext, test_predict)\n",
    "\n",
    "print(mse_train,mse_test,r2_train,r2_test)\n",
    "\n",
    "print('train score : ',r2_train)\n",
    "print('test score : ',r2_test)\n",
    "print('train mse : ', mse_train)\n",
    "print('test mse : ', mse_test)\n",
    "print(\"Run time : \",(endtime - starttime).seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
